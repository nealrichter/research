# Annotated Courses

This file contains a list of courses with their summaries and instructors, organized by category.

# University Classes

## U of Utah

### CS 6340 - Natural Language Processing
[https://utah-cs6340-nlp.notion.site](https://utah-cs6340-nlp.notion.site)

CS 6340, "Natural Language Processing," is a graduate-level course at the University of Utah's School of Computing. It covers algorithms and methods for building computational models of natural language understanding, including syntactic analysis, semantic representations, discourse analysis, and statistical and corpus-based methods for text processing and knowledge acquisition. The course also explores applications of NLP like information extraction, question answering, and machine translation. A recent offering of the course focuses on solving NLP problems with large language models (LLMs), covering their development, foundations like the transformer architecture, transfer learning, instruction following, and alignment, as well as advanced topics such as multilingual, multimodal, safe, and efficient LLMs.

**Instructor:** Ana Marasović

### Mechanistic Interpretability Seminar
[https://utah-mechanistic-interpretability.notion.site](https://utah-mechanistic-interpretability.notion.site)

The University of Utah offers a "Mechanistic Interpretability Seminar" taught by Assistant Professor Ana Marasović. This seminar focuses on the emerging field of mechanistic interpretability, which aims to understand the internal workings of large language models and other complex neural networks. While the search results don't provide a detailed syllabus, the seminar is part of the university's broader AI-related course offerings, which include PhD and MS tracks in Artificial Intelligence and a Deep Learning Certificate.

**Instructor:** Ana Marasović

### Machine Learning (CS 6350, DS 4350)
[https://svivek.com/teaching/machine-learning/spring2025/lectures.html](https://svivek.com/teaching/machine-learning/spring2025/lectures.html)

This course presents a tentative lecture schedule for a Machine Learning course (CS 6350, DS 4350) in Spring 2025. The curriculum spans a wide array of topics, beginning with an introduction to supervised learning, decision trees, and linear models. Subsequent lectures delve into online learning, the Perceptron algorithm, and computational learning theory, including PAC learning and VC dimension. The course also covers advanced machine learning techniques such as boosting, Support Vector Machines (SVMs), stochastic gradient descent, and risk minimization. Further topics include Bayesian learning, logistic regression, neural networks, and nearest neighbor classification. Practical aspects of machine learning are integrated through tutorials on tools like `sklearn` and guidance on building ML applications. The course structure includes homework assignments, project checkpoints, a midterm exam, and a final exam.

**Instructor:** Not explicitly listed.

## Duke

### Explainable AI (XAI) Specialization
[https://www.coursera.org/specializations/explainable-artificial-intelligence-xai](https://www.coursera.org/specializations/explainable-artificial-intelligence-xai)

The Explainable AI (XAI) Specialization by Duke University on Coursera focuses on building ethical and transparent AI systems. It teaches professionals to master explainability techniques and ethical AI development to create trustworthy machine learning solutions. The program, a 3-course series, covers XAI concepts, interpretable machine learning, and advanced explainability techniques for large language models (LLMs) and computer vision models, utilizing hands-on Python labs. It is designed for individuals with a basic to intermediate understanding of machine learning.

**Instructor:** Brinnae Bent, PhD.

## Boston College

### MT875: Mechanistic Interpretability
[https://sites.google.com/bc.edu/eli-grigsby/mt875-mechanistic-interpretability](https://sites.google.com/bc.edu/eli-grigsby/mt875-mechanistic-interpretability)

The course "MT875: Mechanistic Interpretability" for Fall 2024, taught by Eli Grigsby, delves into the geometric aspects of deep learning theory. It investigates how human-interpretable concepts are represented in data encodings' geometry and their interaction with neural network components. A key focus is on the mechanistic interpretability of transformers, the architecture behind large language models. Topics include the geometry and combinatorics of feedforward ReLU neural networks, their universal approximation capabilities, the superposition hypothesis, sequence-to-sequence architectures, geometric distortion, and symmetries in overparameterized ReLU networks. The course is aimed at graduate students and advanced undergraduates in mathematics and theoretical computer science, requiring a strong foundation in undergraduate mathematics and preferably graduate-level geometry and topology. Coursework involves either a lecture/tutorial or experimental work with visualizations.

**Instructor:** Eli Grigsby

## UMass Amherst

### CS 685, Advanced Natural Language Processing
[https://people.cs.umass.edu/~miyyer/cs685/](https://people.cs.umass.edu/~miyyer/cs685/)

This page describes CS 685, Advanced Natural Language Processing, offered in Spring 2024 at UMass Amherst. The course focuses on deep learning methods for NLP, particularly large language models, neural language models, and transfer learning. It covers modeling architectures, training objectives, and downstream tasks like text classification, question answering, and text generation. Intended for graduate students in computer science and linguistics familiar with machine learning fundamentals, the coursework includes reading research papers, programming assignments, and a final project. Lectures are livestreamed on YouTube.

**Instructors:**
*   Instructor: Mohit Iyyer
*   TAs: Chau Pham, Yekyung Kim, Katherine Thai (remote), Saurabh Bajaj

## Berekely

### CS294/194-280 Advanced Large Language Model Agents
[https://rdi.berkeley.edu/adv-llm-agents/sp25](https://rdi.berkeley.edu/adv-llm-agents/sp25)

The CS294/194-280 "Advanced Large Language Model Agents" course for Spring 2025 at UC Berkeley focuses on enhancing LLM agents' capabilities in complex reasoning, planning, mathematics, code generation, and program verification. The curriculum covers advanced inference and post-training techniques, agentic workflows, tool use, and applications in theorem proving and autoformalization. Students are encouraged to enroll via CalCentral and join the waitlist if full. Prerequisites include a basic understanding of Machine Learning and Deep Learning. Grading varies by unit enrollment, requiring weekly reading summaries, and either an article, lab assignment, or a project (application or research track).

**Instructors:**
*   Dawn Song (Professor, UC Berkeley)
*   Xinyun Chen (Research Scientist, Google DeepMind)
*   Kaiyu Yang (Research Scientist, Meta FAIR)
**Teaching Staff:** Alex Pan, Tara Pande, Ashwin Dara, Jason Yan.

### Agentic AI MOOC
[https://agenticai-learning.org/f25](https://agenticai-learning.org/f25)

The Agentic AI MOOC, Fall 2025, is a course built upon previous LLM Agents MOOCs, focusing on the new frontier of Agentic AI and its role in intelligent task automation and personalization. The curriculum covers fundamental concepts like LLM foundations, reasoning, planning, agentic frameworks, and infrastructure, alongside applications in code generation, robotics, and web automation. The course also addresses limitations and risks of current LLM agents. Students can earn completion certificates through various tiers by completing quizzes, a written article, and participating in the AgentX-AgentBeats competition.

**Instructors and Guest Speakers:**
*   Instructor: Dawn Song (Professor, UC Berkeley)
*   Co-instructor: Xinyun Chen (Research Scientist, Meta)
*   Guest Speakers: Yann Dubois (OpenAI), Yangqing Jia (NVIDIA), Jiantao Jiao (NVIDIA), Weizhu Chen (Microsoft), Noam Brown (OpenAI), Sida Wang (Meta), James Zou (Stanford), Clay Bavor (Sierra), Oriol Vinyals (Google DeepMind), Peter Stone (Sony AI, UT Austin).

## MIT

### 6.861* Quantitative Methods for NLP – Advanced NLP
[https://mit-6861.github.io/syllabus](https://mit-6861.github.io/syllabus)

The 6.861* Quantitative Methods for NLP – Advanced NLP course, offered in Fall 2024, focuses on using computers to understand, process, and leverage digitized text information. It covers advanced machine learning and deep learning approaches for popular NLP tasks, with a strong emphasis on original research through a group project. Students will learn theoretical concepts, write programming solutions using Python and PyTorch, and conduct substantial NLP research, including critically reading papers and executing novel ideas. Prerequisites include basic machine learning, probability and statistics, multivariable calculus, linear algebra, algorithms, and Python programming with object-oriented experience. The course structure involves lectures, three homework assignments (30%), a midterm exam (20%), and a significant research project (45%) culminating in a paper, poster, and presentation. Special topic responses for guest lectures account for 5% of the grade. Academic honesty is strictly enforced, prohibiting the use of generative AI and requiring individual work on homework.

**Instructors:**
*   Jacob Andreas
*   Chris Tanner

### 6.S087: Foundation Models & Generative AI
[https://www.futureofai.mit.edu](https://www.futureofai.mit.edu)

The "MIT FUTURE OF AI" website details a non-technical lecture series, 6.S087: Foundation Models & Generative AI, focusing on recent advancements in AI, specifically foundation models and generative AI. The course explores topics like LLMs, agents, text-to-video, self-driving cars, and DNA to proteins, examining their practical and foundational implications across science and business. It is accessible to all backgrounds. The lectures, which began on January 9th, are held on Tuesdays and Thursdays.

**Instructor:** Rickard Brüel Gabrielsson, an MIT researcher and entrepreneur specializing in foundation models and generative AI.

## Harvard

### Explainable AI Course: From Simple Predictors to Complex Generative Models
[https://interpretable-ml-class.github.io/](https://interpretable-ml-class.github.io/)

This Harvard University Spring 2023 course, "Explainable AI Course: From Simple Predictors to Complex Generative Models," focuses on the emerging field of eXplainable Artificial Intelligence (XAI). The curriculum emphasizes the importance of understanding machine learning model behavior, especially in high-stakes domains like healthcare and finance, to ensure reliable decision-making. The course delves into seminal position papers, explores the concept of explainability from various end-user perspectives (e.g., doctors, ML researchers), and discusses different classes of interpretable models and post-hoc explanations. These include rule-based and prototype-based models, feature attributions, counterfactual explanations, and mechanistic interpretability. Furthermore, the course examines the intricate connections between explainability and crucial aspects such as fairness, robustness, and privacy. It also incorporates the latest research on interpreting large generative models, including large language models like GPT-3 and diffusion models like DALLE 2, highlighting the unique opportunities and challenges associated with these advanced systems. Students are expected to have a strong foundation in basic linear algebra, probability, algorithms, and machine learning. Proficiency in programming and software engineering, particularly with Python, numpy, and sklearn, is also a prerequisite for the course.

**Instructors:**
*   Hima Lakkaraju
*   Ike Lage
*   Jiaqi Ma
*   Suraj Srinivas

# AI Training Org

## Weights and Biases

### Training and fine-tuning LLMs
[https://wandb.ai/site/courses/training-fine-tuning-llms/](https://wandb.ai/site/courses/training-fine-tuning-llms/)

The Weights & Biases course "Training and fine-tuning LLMs" is a free, four-hour program designed for machine learning engineers, data scientists, researchers, and NLP enthusiasts. It covers the fundamentals, architecture, training techniques, and fine-tuning methods of large language models, including practical recipes and cutting-edge techniques like LoRA and Prefix Tuning. The curriculum includes foundations, evaluation, data, and training/fine-tuning techniques.

**Instructors:**
*   Darek Kłeczek: MLE at Weights & Biases
*   Jonathan Frankle: Chief Scientist at MosaicML
*   Weiwei Yang: Principal SDE Manager at Microsoft Research
*   Mark Saroufim: PyTorch Engineer at Meta

### Model CI/CD
[https://wandb.ai/site/courses/model-ci-cd/](https://wandb.ai/site/courses/model-ci-cd/)

The "Model CI/CD" course by Weights & Biases provides concepts, best practices, and tools to manage the end-to-end model lifecycle, focusing on overcoming model chaos, automating workflows, and ensuring governance. The free 2.5-hour course covers creating enterprise-level model management systems, automating with Webhooks, and evaluating models with metrics. The curriculum includes model management, automations, LLM case studies, enterprise applications, and a course project.

**Instructor:** Hamel Husain, Founder of Parlance Labs.

### Effective MLOps: Model development
[https://wandb.ai/site/courses/effective-mlops/](https://wandb.ai/site/courses/effective-mlops/)

The "Effective MLOps: Model development" course by Weights & Biases is a free, 4-hour program designed to help build production-ready ML pipelines. It focuses on establishing a disciplined, flexible, and collaborative MLOps system to accelerate model development, enhance productivity, ensure reproducibility, and facilitate faster iteration of models. The curriculum covers building an end-to-end prototype, moving beyond baseline models, and comprehensive model evaluation, aiming to streamline the process of bringing machine learning models into production.

**Instructors:**
*   Hamel Husain (Founder, Parlance Labs)
*   Darek Kłeczek (MLE, Weights & Biases)
*   Thomas Capelle (AI Engineer, Weights & Biases)

## DeepLearning.AI

### Attention in Transformers: Concepts and Code in PyTorch
[https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/)

This DeepLearning.AI short course, "Attention in Transformers: Concepts and Code in PyTorch," explains the attention mechanism crucial to Large Language Models (LLMs) like ChatGPT. The course, approximately 1 hour and 6 minutes long with 11 video lessons and 3 code examples, covers how attention converts token embeddings into context-aware embeddings. It delves into Query, Key, and Value matrices, differentiating between self-attention, masked self-attention, and cross-attention, and how multi-head attention scales the algorithm. Participants will learn the mathematical foundations and implement these concepts in PyTorch. The course is designed for individuals with basic Python knowledge interested in understanding the inner workings of LLMs.

**Instructor:** Josh Starmer, Founder and CEO of StatQuest.

### Agentic AI
[https://www.deeplearning.ai/courses/agentic-ai/](https://www.deeplearning.ai/courses/agentic-ai/)

The DeepLearning.AI "Agentic AI" course focuses on building advanced AI systems that utilize Large Language Models (LLMs) for complex, multi-step tasks. It moves beyond single-response prompts to teach agentic workflows, enabling AI to plan, execute iteratively, and refine outputs through reflection and tool integration. The curriculum covers four core design patterns: Reflection, where AI self-critiques and improves; Tool Use, connecting AI to external resources like databases and APIs; Planning, for breaking down complex tasks; and Multi-Agent systems, coordinating specialized AIs. The course emphasizes practical Python implementation, starting with foundational principles before exploring frameworks. It also addresses evaluating and optimizing AI systems, including performance metrics, error analysis, and production deployment. This intermediate-level course is for software developers proficient in Python and familiar with LLMs and APIs, aiming to teach them how to transform business processes into agentic workflows. A certificate is awarded upon completion.

**Instructor:** Andrew Ng

### Finetuning Large Language Models
[https://www.deeplearning.ai/short-courses/finetuning-large-language-models/](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)

The DeepLearning.AI short course, "Finetuning Large Language Models," provides an intermediate-level introduction to finetuning Large Language Models (LLMs). The course, lasting 1 hour and 25 minutes, covers the fundamentals of finetuning, how it differs from prompt engineering, and when to apply each technique. Learners will gain practical experience in preparing data for finetuning, training, and evaluating LLMs to enable them to learn specific styles, forms, or new knowledge. The course includes 9 video lessons and 6 code examples.

**Instructor:** Sharon Zhou, who is the Vice President of Artificial Intelligence at AMD.

### Multi AI Agent Systems with crewAI
[https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewAI/](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewAI/)

This DeepLearning.AI short course, "Multi AI Agent Systems with crewAI," teaches how to design and prompt teams of AI agents to outperform single LLMs for complex, multi-step tasks. The course, lasting 2 hours and 41 minutes, covers using the open-source `crewAI` library to automate business processes like tailoring resumes, automating customer support, and event planning. It delves into key principles such as role-playing, memory, tools, focus, guardrails, and cooperation among agents.

**Instructor:** João Moura, the founder and CEO of crewAI.

### AI Agentic Design Patterns with AutoGen
[https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/)

The DeepLearning.AI short course "AI Agentic Design Patterns with AutoGen" teaches participants to build and customize multi-agent systems using the AutoGen framework. The course covers implementing agentic design patterns such as reflection, tool use, planning, and multi-agent collaboration. It includes practical examples like creating a two-agent chat, a customer onboarding experience, a blog post using reflection, a conversational chess game, and agents for financial analysis and stock report generation. The course is designed for individuals with basic Python experience interested in automating complex workflows with AI agents.

**Instructors:**
*   Chi Wang (Principal Researcher at Microsoft Research)
*   Qingyun Wu (Assistant Professor at Penn State University)

### AI Agents in LangGraph
[https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)

This DeepLearning.AI short course, "AI Agents in LangGraph," teaches developers to build highly controllable AI agents using LangGraph, an extension of the LangChain framework. The course covers building agents from scratch, implementing them with LangGraph components, and integrating agentic search for enhanced knowledge. It also delves into persistence for state management and incorporating human-in-the-loop systems. The curriculum includes practical applications like developing an essay-writing agent. The course is designed for individuals with intermediate Python knowledge.

**Instructors:**
*   Harrison Chase (Co-Founder and CEO of LangChain)
*   Rotem Weiss (Co-founder and CEO of Tavily)

### Post-training of LLMs
[https://learn.deeplearning.ai/courses/post-training-of-llms/](https://learn.deeplearning.ai/courses/post-training-of-llms/)

The DeepLearning.AI course "Post-training of LLMs" focuses on fine-tuning pre-trained Large Language Models (LLMs) for specific tasks, which is a faster and more cost-effective process than pre-training. The course covers three key post-training techniques: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning. SFT involves training models on labeled prompt-response pairs to enable them to follow instructions. DPO teaches models by presenting both good and bad answers for the same prompt, pushing the model towards preferred responses. Online Reinforcement Learning updates models based on the quality of generated answers, utilizing reward functions and algorithms like Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO). Learners will gain practical experience by downloading and post-training a pre-trained model themselves, applying these techniques to tasks such as instruction following, identity modification, and solving math problems.

**Instructors:**
*   Banghua Zhu (Assistant Professor at the University of Washington, co-founder of NexusFlow)
*   Oleksii Kuchaiev (Nvidia)
*   Jiantao Jiao (UC Berkeley)
*   Esmaeil Gargari (DeepLearning.AI)

## Neel Nanda

### A Barebones Guide to Mechanistic Interpretability Prerequisites
[https://www.neelnanda.io/mechanistic-interpretability/prereqs](https://www.neelnanda.io/mechanistic-interpretability/prereqs)

The article "A Barebones Guide to Mechanistic Interpretability Prerequisites" by Neel Nanda and co-authored by Jess Smith, outlines essential skills for understanding and contributing to mechanistic interpretability in AI. It aims to demystify the field, suggesting that many overestimate the required core skills. The guide covers core mathematical concepts like linear algebra (basis, change of basis, vector spaces, matrices as linear maps, SVD, eigenvalues/eigenvectors) and probability basics (distributions, expected value, log likelihood, random variables, central limit theorem). Calculus basics, including gradients and the chain rule for understanding backpropagation, are also highlighted. For coding, the guide emphasizes Python basics, NumPy, and PyTorch, recommending resources like Al Sweigart's books and Andrej Karpathy's neural net video. It specifically advises learning `einops` and `einsum` for efficient tensor manipulation in PyTorch. A deep understanding of transformer architecture is deemed crucial, with Neel Nanda's own tutorials and explainer as recommended resources. The article encourages hands-on coding and direct engagement with research ideas over mastering every "nice-to-have" skill.

**Instructors:**
*   Neel Nanda
*   Jess Smith

### How To Become A Mechanistic Interpretability Researcher
[https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher)

The article "How To Become A Mechanistic Interpretability Researcher" by Neel Nanda provides a comprehensive guide for aspiring researchers in mechanistic interpretability (mech interp). Nanda emphasizes a "learn by doing" philosophy, advocating for minimal initial learning followed by immediate engagement in research. The process is broken down into three stages: "Learning the Ropes" (≤1 month) focusing on breadth-first basics like coding transformers, understanding key techniques, and using LLMs as learning tools; "Mini-projects" (1-5 days each for 2-4 weeks) to practice fast-feedback loop skills like exploration and understanding; and "Towards full projects" (1-2 week sprints) to develop deeper research skills, critical skepticism, and effective write-ups. The guide stresses the importance of practical experience, extensive use of LLMs for learning and coding, and proactive mentorship seeking. It also discusses the evolving landscape of mech interp, advising against fads and highlighting promising new directions like downstream tasks and model organisms for safety research.

**Instructor:** Neel Nanda

## BlueDot Impact

### AI Alignment: Unit 6 | Resources: Mechanistic interpretability
[https://bluedot.org/courses/alignment/6/1](https://bluedot.org/courses/alignment/6/1)

The Bluedot Impact AI Alignment course, Unit 6, focuses on "Mechanistic Interpretability," which explores methods to understand the internal reasoning and decision-making processes of AI models. The unit emphasizes the growing need for transparency in advanced AI systems. Key topics include analyzing models' learned representations and weights through techniques like circuit analysis, and addressing challenges such as superposition using dictionary learning. By the end of a unit, participants should grasp what mechanistic interpretability is, its relevance to AI Alignment, and core concepts like circuits and superposition.

**Instructors/Authors mentioned in the resources:**
*   Sarah Hastings-Woodhouse
*   Chris Olah
*   Nick Cammarata
*   Ludwig Schubert
*   Scott Alexander
*   Neel Nanda
*   Charbel-Raphael Segerie
*   Callum McDougall
*   Nelson Elhage
*   Tristan Hume
*   Catherine Olsson
*   Ryan Greenblatt
*   Buck Shlegeris
*   Trenton Bricken
*   Adly Templeton
*   Joshua Batson
*   Kevin Wang
*   Alexandre Variengien
*   Arthur Conmy
*   Gabriel Goh
*   Chelsea Voss
*   Tom Conerly
*   Hamish Doodles
*   Daniel Filan
*   Kevin Meng
*   David Bau
*   Alex Andonian
*   Jesse Hoogland
*   Alexander Gietelink
*   Daniel Murfet
*   Tolga Bolukbasi
*   Adam Pearce
*   Ann Yuan
*   Adrià Garriga-Alonso
*   Nicholas Goldowsky-Dill
*   Paul Christiano
*   Ajeya Cotra
*   Mark Xu
*   Collin Burns
*   Haotian Ye
*   Dan Klein
*   Kenneth Li
*   Oam Patel
*   Casper
*   Robert Wiblin
*   Keiran Harris
*   Evan Hubinger
*   Steven Byrnes
*   Steven Liu
*   Tongzhou Wang
*   Been Kim
*   Martin Wattenberg
*   Justin Gilmer

### Technical AI Safety Course
[https://bluedot.org/courses/technical-ai-safety](https://bluedot.org/courses/technical-ai-safety)

The BlueDot Impact Technical AI Safety course aims to equip participants with the knowledge to build safer AI by understanding current safety techniques, identifying gaps, and contributing to the field. The course is designed for ML researchers, software engineers, and policy professionals, offering a structured overview of key safety techniques in approximately 30 hours. It covers topics such as the technical challenges of AI safety, training safer models, a community of individuals working to make AI go well.

**Facilitators:**
*   Neel Nanda (Mech Interp Lead at Google DeepMind)
*   Marius Hobbhahn (CEO at Apollo Research)
*   Richard Ngo (Former OpenAI and DeepMind, AI Alignment Course Designer)
*   Adam Jones (Member of Technical Staff at Anthropic, Former AI safety lead at BlueDot)
*   Juan Felipe Ceron Uribe (AI Alignment Research Engineer at OpenAI)
*   Nikita Ostrovsky (AI Reporter at TIME)
*   Ana Carvalho (Contributor to AI Safety Engineering Taskforce)
*   Sabrina Shih (AI Policy Manager at Responsible AI Institute)
*   Cameron Holmes (Senior Research Manager at MATS)

# Conference Workshop

## Mechanistic Interpretability Workshop (NeurIPS)
[https://mechinterpworkshop.com/](https://mechinterpworkshop.com/)

The Mechanistic Interpretability Workshop at NeurIPS 2025, scheduled for Sunday, December 7, 2025, at the San Diego Convention Centre, focuses on understanding the internal mechanisms of neural networks. The workshop aims to address the challenge of interpreting model decisions, which is crucial for predicting behavior, ensuring reliability, and detecting adversarial actions. It will bring together researchers from academia, industry, and independent research to discuss advancements, build common understanding, and define future directions in mechanistic interpretability. The event will feature keynote speakers and poster sessions, building on the success of the ICML 2024 workshop.

**Keynote Speakers:**
*   Chris Olah (Interpretability Lead and Co-founder, Anthropic)
*   Been Kim (Senior Staff Research Scientist, Google DeepMind)
*   Sarah Schwettmann (Co-founder, Transluce)
**Organizing Committee:**
*   Neel Nanda (Senior Research Scientist, Google DeepMind)
*   Andrew Lee (Post-doc, Harvard)
*   Andy Arditi (PhD Student, Northeastern University)
*   Jemima Jones (Operations Lead)
*   Stefan Heimersheim (Member of Technical Staff, FAR.AI)
*   Anna Soligo (PhD Student, Imperial)
*   Martin Wattenberg (Professor, Harvard University & Principal Research Scientist, Google DeepMind)
*   Atticus Geiger (Lead, Pr(Ai)²R Group)
*   Julius Adebayo (Founder and Researcher, Guide Labs)
*   Kayo Yin (3rd year PhD student, UC Berkeley)
*   Fazl Barez (Senior Research Fellow, Oxford Martin AI Governance Initiative)
*   Lawrence Chan (Researcher, METR)
*   Matthew Wearden (London Director, MATS)

## ICML 2024
[https://icml2024mi.pages.dev/](https://icml2024mi.pages.dev/)

The ICML 2024 Mechanistic Interpretability Workshop, held on July 27th in Vienna, focuses on understanding the internal workings of neural networks by reverse engineering their algorithms into human-understandable mechanisms. The workshop aims to unite researchers from industry and academia to discuss progress, challenges, and future goals in this rapidly evolving field. It features talks, oral presentations, poster sessions, and a panel discussion, alongside a hands-on tutorial using the TransformerLens package. The event also awarded prizes for top papers in mechanistic interpretability research.

**Instructors/Speakers/Panelists include:**
*   David Bau (Northeastern University)
*   Asma Ghandeharioun (Google DeepMind)
*   Chris Olah (Anthropic)
*   Naomi Saphra (Harvard University)
*   Atticus Geiger (Pr(Ai)2R Group)
*   Stella Biderman (EleutherAI)
*   Arthur Conmy (Google DeepMind)
*   Fazl Barez (University of Oxford)
*   Mor Geva (Tel Aviv University, Google Research)
*   Lawrence Chan (UC Berkeley)
*   Kayo Yin (UC Berkeley)
*   Neel Nanda (Google DeepMind)
*   Max Tegmark (MIT)